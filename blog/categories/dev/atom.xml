<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: dev | Solarex's Blog]]></title>
  <link href="http://Solarex.github.io/blog/categories/dev/atom.xml" rel="self"/>
  <link href="http://Solarex.github.io/"/>
  <updated>2019-10-15T23:51:54+08:00</updated>
  <id>http://Solarex.github.io/</id>
  <author>
    <name><![CDATA[Solarex]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[如何精确测量一个ViewTree的绘制时长]]></title>
    <link href="http://Solarex.github.io/blog/2019/10/15/how-to-measure-viewtree-time-cost-exactly/"/>
    <updated>2019-10-15T23:44:00+08:00</updated>
    <id>http://Solarex.github.io/blog/2019/10/15/how-to-measure-viewtree-time-cost-exactly</id>
    <content type="html"><![CDATA[<p>我们都知道在Android中View树的绘制需要经历measure,layout,draw三个流程，那如何精确测量一个View树的绘制时长呢？</p>

<!-- more -->

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[谁唤醒了MessageQueue中的nativePollOnce]]></title>
    <link href="http://Solarex.github.io/blog/2019/10/10/who-awake-nativepollonce-in-messagequeue/"/>
    <updated>2019-10-10T00:00:00+08:00</updated>
    <id>http://Solarex.github.io/blog/2019/10/10/who-awake-nativepollonce-in-messagequeue</id>
    <content type="html"><![CDATA[<p>在看Handler相关源码的时候，我们都知道在MessageQueue的nativePollOnce这里会阻塞，那什么时候主线程从这里被唤醒呢？</p>

<!-- more -->

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[单例模式]]></title>
    <link href="http://Solarex.github.io/blog/2019/08/12/singleton-pattern/"/>
    <updated>2019-08-12T07:34:00+08:00</updated>
    <id>http://Solarex.github.io/blog/2019/08/12/singleton-pattern</id>
    <content type="html"><![CDATA[<p>本文主要讲解单例模式的各种实现，以及对反射和反序列化攻击的防御。</p>

<!-- more -->


<p>单例模式的实现主要注意以下几点：</p>

<ul>
<li>私有构造器</li>
<li>线程安全</li>
<li>延迟加载</li>
<li>序列化和反序列化安全</li>
<li>反射攻击</li>
</ul>


<p>下面依次看各种实现：</p>

<h3>饿汉式</h3>

<p>饿汉式顾名思义就是在类加载的时候直接实例化单例。由于是在类加载的时候进行，所以可能拖慢启动速度，而且由于是static变量，生命周期比较长，比较占用内存，特别是如果实例化了而又没有用到的话就白白浪费了内存。</p>

<h4>对反序列化的防御</h4>

<p><code>ObjectInputStream</code>的<code>readObject</code>方法时会判断如果一个类实现了<code>Serializable</code>接口并且定义了<code>readResolve</code>方法的话会调用<code>readResolve</code>方法来进行反序列化，因此可以在<code>readResolve</code>方法里面进行处理。</p>

<h4>对反射的防御</h4>

<p>由于实例在类加载的时候已经实例化，因此可以在构造函数中对实例进行判断，如果不为<code>null</code>直接抛出<code>RuntimeException</code>来阻止反射创建实例。</p>

<p>具体代码可见<a href="https://github.com/flyfire/DesignPatternsLearning/blob/master/src/main/java/com/solarexsoft/designpatterns/pattern/creational/singleton/HungrySingleton.java">HungrySingleton.java</a></p>

<h3>懒汉式</h3>

<p>顾名思义就是在用到的时候再进行实例化，同时使用static synchronized方法来保证线程安全。但是这种实现方式每次进<code>getInstance</code>方法都要获取锁，实际上只有实例化的时候需要获取锁，其他时候直接返回就好了，对这种方案的优化就是下面的双重检查锁方案，现在我们暂时先看这种方案。</p>

<h4>对反序列化的防御</h4>

<p>同样，在<code>readResolve</code>方法中进行处理。</p>

<h4>对反射的防御</h4>

<p>由于实例只有在调用<code>getInstance</code>方法之后才进行实例化，因此无法进行防御。</p>

<p>具体代码见<a href="https://github.com/flyfire/DesignPatternsLearning/blob/master/src/main/java/com/solarexsoft/designpatterns/pattern/creational/singleton/LazySingleton.java">LazySingleton.java</a></p>

<h3>双重检查锁</h3>

<p>如上所说，双重检查锁是为了解决加锁过重问题而产生的方案。这种方案有个问题，就是调用new来实例化一个对象并不是原子操作，其中初始化对象这步和将实例指向分配的内存地址这步可能会重排序，在某种情况下，可能出现其他线程使用一个未完全初始化的对象的问题。解决这个问题有两种方案，一是禁止重排序，通过在变量时加上volatile修饰符来解决，二是允许重排序，但是这种重排序不能对外界线程可见。双重检查锁使用的是volatile修饰变量来解决这种问题，使用第二种方案来解决问题的方式是我们下面要讲的静态内部类方式。</p>

<h4>对反序列化的防御</h4>

<p>同样，在<code>readResolve</code>方法中进行处理。</p>

<h4>对反射的防御</h4>

<p>和懒汉式一样，无法防御。</p>

<p>具体代码见<a href="https://github.com/flyfire/DesignPatternsLearning/blob/master/src/main/java/com/solarexsoft/designpatterns/pattern/creational/singleton/LazyDoubleCheckSingleton.java">LazyDoubleCheckSingleton.java</a></p>

<h3>静态内部类</h3>

<p>这种方式被推荐是因为使用了Class对象的初始化锁来保证指令重排序对外界线程不可见，实现方式很优雅。</p>

<h4>对反序列化的防御</h4>

<p>在<code>readResolve</code>方法中处理即可。</p>

<h4>对反射的防御</h4>

<p>和饿汉式一样，也是在类加载时初始化实例，因此可以在构造函数中进行判断并抛出异常。</p>

<p>具体代码见<a href="https://github.com/flyfire/DesignPatternsLearning/blob/master/src/main/java/com/solarexsoft/designpatterns/pattern/creational/singleton/StaticInnerClassSingleton.java">StaticInnerClassSingleton.java</a></p>

<h3>Enum实现单例</h3>

<p>EffectiveJava中告诉我们可以使用Enum来实现单例，并且这种方式实现的单例天然具有防御反序列化和反射攻击的优势。下面我们看他是如何实现的。</p>

<h4>对反序列化的防御</h4>

<p><code>ObjectInputStream</code>方法中<code>readEnum</code>直接调用了<code>Enum.valueOf</code>方法来进行Enum的反序列化，因此得到的是同样的实例。</p>

<h4>对反射的防御</h4>

<p><code>Constructor</code>的<code>newInstance</code>方法会对要反射类的<code>modifiers</code>进行判断，如果是<code>Enum</code>会直接抛出<code>IllegalArgumentException</code>异常。</p>

<p>具体代码见<a href="https://github.com/flyfire/DesignPatternsLearning/blob/master/src/main/java/com/solarexsoft/designpatterns/pattern/creational/singleton/EnumSingleton.java">EnumSingleton.java</a></p>

<p>从<a href="https://github.com/flyfire/DesignPatternsLearning/blob/master/src/main/java/com/solarexsoft/designpatterns/pattern/creational/singleton/EnumSingleton.jad">反编译后的代码</a>来看，Enum实现单例和饿汉式类似，都是在类加载静态初始化时初始化了实例对象。</p>

<h3>Kotlin简单单例</h3>

<p>kotlin实现单例很简单，声明为<code>object</code>即可。具体代码可见<a href="https://github.com/flyfire/DesignPatternsLearning/blob/master/src/main/java/com/solarexsoft/designpatterns/pattern/creational/singleton/KotlinSimpleSingleton.kt">KotlinSimpleSingleton.kt</a>。反编译代码可以看到，实际实现方式和饿汉式类似。</p>

<p>```java
public final class Solarex {
   public static final Solarex INSTANCE;</p>

<p>   static {</p>

<pre><code>  Solarex var0 = new Solarex();
  INSTANCE = var0;
</code></pre>

<p>   }
}
```</p>

<h3>Kotlin带参数的单例</h3>

<p>一般构造单例对象的时候需要注入一些对象参数，参考<a href="https://medium.com/@BladeCoder/kotlin-singletons-with-argument-194ef06edd9e">这篇文章</a>实现了一下带参数的kotlin单例。具体代码可见<a href="https://github.com/flyfire/DesignPatternsLearning/blob/master/src/main/java/com/solarexsoft/designpatterns/pattern/creational/singleton/KotlinSingletonWithArguments.kt">KotlinSingletonWithArguments.kt</a>。其实实现仍是双重检查+volatile。</p>

<h3>reference</h3>

<ul>
<li><p><a href="https://github.com/flyfire/DesignPatternsLearning/tree/master/src/main/java/com/solarexsoft/designpatterns/pattern/creational/singleton">Singleton</a></p></li>
<li><p><a href="https://medium.com/@BladeCoder/kotlin-singletons-with-argument-194ef06edd9e">Kotlin singletons with argument</a></p></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Lock-free multithreading with atomic operations]]></title>
    <link href="http://Solarex.github.io/blog/2019/08/10/lock-free-multithreading-with-atomic-operations/"/>
    <updated>2019-08-10T10:16:00+08:00</updated>
    <id>http://Solarex.github.io/blog/2019/08/10/lock-free-multithreading-with-atomic-operations</id>
    <content type="html"><![CDATA[<p>清空Reeder发现internal/pointers一系列关于多线程的文章，感觉很不错，转载在此。</p>

<p>这是系列第三篇，期待作者继续更新~</p>

<p>Synchronizing threads at a lower level.</p>

<!-- more -->


<p>The Greek word &ldquo;atom&rdquo; (ἄτομος; atomos) means <em>uncuttable</em>. A task performed by a computer is said to be <strong>atomic</strong> when it is not divisible anymore: it can&rsquo;t be broken into smaller steps.</p>

<p><em>Atomicity</em> is an important property of multithreaded operations: since they are indivisible, there is no way for a thread to <em>slip through</em> an atomic operation concurrently performed by another one. For example, when a thread atomically writes on shared data no other thread can read the modification half-complete. Conversely, when a thread atomically reads from shared data, it sees the value as it appeared at a single moment in time. In other words, there is no risk of <a href="https://www.internalpointers.com/post/gentle-introduction-multithreading#data-races"><strong>data races</strong></a>.</p>

<p>In the <a href="https://www.internalpointers.com/post/introduction-thread-synchronization">previous chapter</a> I have introduced the so-called <strong>synchronization primitives</strong>, the most common tools for thread synchronization. They are used, among other things, to provide atomicity to operations that deal with data shared across multiple threads. How? They simply allow a single thread to do its concurrent job, while others are blocked by the operating system until the first one has finished. The rationale is that a blocked thread does no harm to others. Given their ability to freeze threads, such synchronization primitives are also known as <strong>blocking mechanisms</strong>.</p>

<p>Any blocking mechanism seen in the previous chapter will work great for the vast majority of your applications. They are fast and reliable if used correctly. However, they introduce some drawbacks that you might want to take into account:</p>

<ul>
<li>they block other threads — a dormant thread simply waits for the wakeup signal, doing nothing: it could be wasting precious time;</li>
<li>they could hang your application — if a thread holding a lock to a synchronization primitive crashes for whatever reason, the lock itself will be never released and the waiting threads will get stuck forever;</li>
<li>you have little control over which thread will sleep — it&rsquo;s usually up to the operating system to choose which thread to block. This could lead to an unfortunate event known as <strong>priority inversion</strong>: a thread that is performing a very important task gets blocked by another one with a lower priority.</li>
</ul>


<p>Most of the time you don&rsquo;t care about these issues as they won&rsquo;t affect the correctness of your programs. On the other hand, sometimes having threads always up and running is desirable, especially if you want to take advantage of multi-processor/multi-core hardware. Or maybe you can&rsquo;t afford a system that could get stuck on a dead thread. Or again, the priority inversion problem looks too dangerous to ignore.</p>

<h3>Lock-free programming to the rescue</h3>

<p>The good <a href="news:">news:</a> there is another way to control concurrent tasks in your multithreaded app, in order to prevent points 1), 2) and 3) seen above. Known as <strong>lock-free programming</strong> or <strong>lockless programming</strong>, it&rsquo;s a technique to safely share changing data between multiple threads without the cost of locking and unlocking them.</p>

<p>The bad <a href="news:">news:</a> this is low-level stuff. Way lower than using the traditional synchronization primitives like mutexes and semaphores: this time we will get closer to the metal. Despite this, I find lock-free programming a good mental challenge and a great opportunity to better understand how a computer actually works.</p>

<p>Lock-free programming relies upon <strong>atomic instructions</strong>, operations performed directly by the CPU that occur atomically. Being the foundation of lock-free programming, in the rest of this article I will introduce atomic instructions first, then I will show you how to leverage them for concurrency control. Let&rsquo;s get started!</p>

<h3>What are atomic instructions?</h3>

<p>Think of any action performed by a computer, say for example displaying a picture on your screen. Such operation is made of many smaller ones: read the file into memory, de-compress the image, light up pixels on the screen and so on. If you recursively zoom into one of those sub-tasks, that is if you break it down into smaller and smaller pieces, you will eventually reach a dead end. The smallest, visible to a human operation performed by a processor is called <strong>machine instruction</strong>, a command executed by the hardware directly.</p>

<center><p><img src="http://Solarex.github.io/images/software-hardware-layers.png"></p></center>


<p>Depending on the CPU architecture, some machine instructions are atomic, that is they are performed in a single, uncuttable and uninterruptible step. Some others are not atomic instead: the processor does more work under the hood in form of even smaller operations, known as <strong>micro-operations</strong>. Let&rsquo;s focus on the former category: an atomic instruction is a CPU operation that cannot be further broken down. More specifically, atomic instructions can be grouped into two major classes: <strong>store and load</strong> and <strong>read-modify-write (RMW)</strong>.</p>

<h4>Store and load atomic instructions</h4>

<p>The building blocks any processor operates on: they are used to write (<strong>store</strong>) and read (<strong>load</strong>) data in memory. Many CPU architectures guarantee that these operations are atomic by nature, under some circumstances. For example, processors that implement the <a href="https://en.wikipedia.org/wiki/X86">x86 architecture</a> feature the <code>MOV</code> instruction, which reads bytes from memory and gives them to the CPU. This operation is guaranteed to be atomic if performed on <a href="https://www.ibm.com/support/knowledgecenter/en/SSUFAU_1.0.0/com.ibm.ent.pl1.zos.doc/lr/alnmnt.html"><strong>aligned</strong></a> data, that is information stored in memory in a way that makes it easy for the CPU to read it in a single shot.</p>

<h4>Read-modify-write (RMW) atomic instructions</h4>

<p>Some more complex operations can&rsquo;t be performed with simple stores and loads alone. For example, incrementing a value in memory would require a mixture of at least three atomic load and store instructions, making the outcome non-atomic as a whole. <strong>Read-modify-write</strong> instructions fill the gap by giving you the ability to compute multiple operations in one atomic step. There are many instructions in this class. Some CPU architectures provide them all, some others only a subset. To name a few:</p>

<ul>
<li><a href="https://en.wikipedia.org/wiki/Test-and-set"><strong>test-and-set</strong></a> — writes 1 to a memory location and returns the old value in a single, atomic step;</li>
<li><a href="https://en.wikipedia.org/wiki/Fetch-and-add"><strong>fetch-and-add</strong></a> — increments a value in memory and returns the old value in a single, atomic step;</li>
<li><a href="https://en.wikipedia.org/wiki/Compare-and-swap"><strong>compare-and-swap (CAS)</strong></a> — compares the content of a memory location with a given value and, if they are equal, modifies the contents of that memory location to a new given value.</li>
</ul>


<p>All these instructions perform multiple things in memory in a single, atomic step. This is an important property that makes read-modify-write instructions suitable for lock-free multithreading operations. We will see why in few paragraphs.</p>

<h3>Three levels of atomic instructions</h3>

<p>All the instructions seen above belong to the hardware: they require you to talk directly to the CPU. Working this way is obviously difficult and non-portable, as some instructions might have different name across different architectures. Some operations might not even exist across different processor models! So it is unlikely you will touch these things, unless you are working on very low-level code for a specific machine.</p>

<p>Climbing up to the software level, many operating systems provide their own versions of atomic instructions. Let&rsquo;s call them <strong>atomic operations</strong>, since we are abstracting away from their physical machine counterpart. For example, in Windows you may find the <a href="https://docs.microsoft.com/en-us/windows/desktop/sync/interlocked-variable-access">Interlocked API</a>, a set of functions that handle variables in an atomic manner. MacOS does the same with its <a href="https://developer.apple.com/documentation/kernel/osatomic_h?language=objc">OSAtomic.h</a> header. They surely conceal the hardware implementation, but you are still bound to a specific environment.</p>

<p>The best way to perform portable atomic operations is to rely upon the ones provided by the programming language of choice. In Java for example you will find the <code>java.util.concurrent.atomic</code> package; C++ provides the <code>std::atomic</code> header; Haskell has the <code>Data.Atomics</code> package and so on. Generally speaking, it is likely to find support for atomic operations if a programming language deals with multithreading. This way is up to the compiler (if it&rsquo;s a compiled language) or the virtual machine (if it&rsquo;s an interpreted language) to find the best instructions for implementing atomic operations, whether from the underlying operating system API or directly from the hardware.</p>

<center><p><img src="http://Solarex.github.io/images/atomics-levels.png"></p></center>


<p>For example, GCC — a C++ compiler — usually transforms C++ atomic operations and objects straight into machine instructions. It also tries to emulate a specific operation that doesn&rsquo;t map directly to the hardware with other atomic machine instructions if available. The worst-case scenario: on a platform that doesn&rsquo;t provide atomic operations it may rely upon other blocking strategies, which wouldn&rsquo;t be lock-free, of course.</p>

<h3>Leveraging atomic operations in multithreading</h3>

<p>Let&rsquo;s now see how atomic operations are used. Consider incrementing a simple variable, an task that is not atomic by nature as it is made of three different steps — read the value, increment it, store the new value back. Traditionally, you would regulate the operation with a mutex (pseudocode):</p>

<p>```c
mutex = initialize_mutex()
x     = 0</p>

<p>reader_thread()</p>

<pre><code>mutex.lock()
print(x)
mutex.unlock()
</code></pre>

<p>writer_thread()</p>

<pre><code>mutex.lock()
x++
mutex.unlock()
</code></pre>

<p>```</p>

<p>The first thread that acquires the lock makes progress, while others sit and wait in line until it has finished.</p>

<p>Conversely, the lock-free approach introduces a different pattern: threads are free to run without any impediment, by employing atomic operations. For example:</p>

<p>```c
x = 0</p>

<p>reader_thread()</p>

<pre><code>print(load(x))
</code></pre>

<p>writer_thread()</p>

<pre><code>fetch_and_add(x, 1)
</code></pre>

<p>```</p>

<p>I assume that <code>fetch_and_add()</code> and <code>load()</code> are atomic operations based on the corresponding hardware instructions. As you may notice, nothing is locked here. Multiple threads that call those functions concurrently can all make progress. The atomicity of <code>load()</code> makes sure that no reader thread will read the shared value half-complete, as well as no writer thread will damage it with a partial write thanks to <code>fetch_and_add()</code>.</p>

<h4>Atomic operations in the real world</h4>

<p>Now, this example reveals us an important property of atomic operations: they work only with primitive types — booleans, chars, shorts, ints and so on. On the other hand, actual programs require synchronization for more complex structures like arrays, vectors, objects, vectors of arrays, objects containing arrays, &hellip; . How can we guarantee atomicity on such convoluted entities with simple operations based on primitive types?</p>

<p>Lock-free programming forces you to think out of the box of the usual synchronization primitives. You don&rsquo;t protect a shared resource with atomic operations directly, as you would do with a mutex or a semaphore. Rather, you build <strong>lock-free algorithms</strong> or <strong>lock-free data structures</strong>, based on atomic operations to determine how multiple threads will access your data.</p>

<p>For example, the <em>fetch-and-add</em> operation seen before can be used to make a rudimentary semaphore that, in turn, you would employ to regulate threads. Not surprisingly all the traditional, blocking synchronization entities are based on atomic operations.</p>

<p>People have written countless lock-free data structures like <a href="https://github.com/facebook/folly/blob/master/folly/AtomicHashMap.h">Folly&rsquo;s AtomicHashMap</a>, the <a href="https://www.boost.org/doc/libs/1_70_0/doc/html/lockfree.html">Boost.Lockfree library</a>, multi-producer/multi-consumer <a href="https://github.com/cameron314/concurrentqueue">FIFO queues</a> or algorithms like <a href="https://www.youtube.com/watch?v=rxQ5K9lo034">read-copy-update (RCU)</a> and <a href="https://en.wikipedia.org/wiki/Shadow_paging">Shadow Paging</a> to name a few. Writing these atomic weapons from scratch is hard, let alone making them work correctly. This is why most of the time you may want to employ existing, battle-tested algorithms and structures instead of rolling your owns.</p>

<h3>The compare-and-swap (CAS) loop</h3>

<p>Moving closer to real-world applications, the <strong>compare-and-swap loop</strong> (a.k.a. <strong>CAS loop</strong>) is probably the most common strategy in lock-free programming, whether you are using existing data structures or are writing algorithms from the ground up. It is based on the corresponding <em>compare-and-swap</em> atomic operation and has a nice property: it supports multiple writers. This is an important feature of a concurrent algorithm especially when used in complex systems.</p>

<p>The CAS loop is interesting also because it introduces a recurring pattern in lock-free code, as well as some theoretical concepts to reason about. Let&rsquo;s take a closer look.</p>

<h4>A CAS loop in action</h4>

<p>A CAS function provided by an operating system or a programming language might look like this:</p>

<p><code>c
boolean compare_and_swap(shared_data, expected_value, new_value);
</code></p>

<p>It takes in input a reference/pointer to some shared data, the expected value it currently takes on and the new value you want to apply. The function replaces the current value with the new one (and returns <code>true</code>) only if the value hasn&rsquo;t changed, that is if <code>shared_data.value == expected_value</code>.</p>

<p>In a CAS loop the idea is to repeatedly trying to compare and swap until the operation is successful. On each iteration you feed the CAS function with the reference/pointer, the expected value and the desired one. This is necessary in order to cope with any other writer thread that is doing the same thing concurrently: the CAS function fails if another thread has changed the data in the meantime, that is if the shared data no longer matches the expected value. Multiple writers support!</p>

<p>Suppose we want to replicate the fetch-and-add algorithm seen in the previous snippet with a CAS loop. It would look roughly like this (pseudocode):</p>

<p>```c
x = 0</p>

<p>reader_thread()</p>

<pre><code>print(load(x))
</code></pre>

<p>writer_thread()</p>

<pre><code>temp = load(x)                              // (1)
while(!compare_and_swap(x, temp, temp + 1)) // (2)
</code></pre>

<p>```</p>

<p>In (1) the algorithm loads the existing value of the shared data, then it tries to swap it with the new value until success (2), that is until the CAS function returns <code>true</code>.</p>

<h4>The swapping paradigm</h4>

<p>As said before, the CAS loop introduces a recurring pattern in many lock-free algorithms:</p>

<ul>
<li>create a <em>local copy</em> of the shared data;</li>
<li>modify the local copy as needed;</li>
<li>when ready, update the shared data by <em>swapping</em> it with the local copy created before.</li>
</ul>


<p>Point 3) is the key: the swap is performed atomically through an atomic operation. The dirty job is done <em>locally</em> by the writer thread and then published only when ready. This way another thread can observe the shared data only in two states: either the old one, or the new one. No half-complete or corrupted updates, thanks to the atomic swap.</p>

<p>This is also philosophically different from the locking approach: in a lock-free algorithm threads get in touch only during that tiny atomic swap, running undisturbed and unaware of others for the rest of the time. The point of contact between threads is now shrinked down and limited to the duration of the atomic operation.</p>

<h4>A gentle form of locking</h4>

<p>The <em>spin until success</em> strategy seen above is employed in many lock-free algorithms and is called <strong>spinlock</strong>: a simple loop where the thread repeatedly tries to perform something until successful. It&rsquo;s a form of gentle lock where the thread is up and running — no sleep forced by the operating system, although no progress is made until the loop is over. Regular locks employed in mutexes or semaphores are way more expensive, as the suspend/wakeup cycle requires a lot of work under the hood.</p>

<h4>The ABA problem</h4>

<p>Instructions in lines (1) and (2) are atomic indeed, yet distinct. Another thread might slip through the cracks and change the value of the shared data once has been read in (1). Specifically, it could turn the initial value, say <code>A</code>, into another value, say <code>B</code>, and then bring it back to <code>A</code> right before the compare and swap operation has started in (2). The thread that is running the CAS loop wouldn&rsquo;t notice the change and perform the swap successfully. This is known as the <strong>ABA problem</strong>: sometimes you can easily ignore it if you algorithm is simple as the one above, sometimes you want to prevent it instead as it would introduce subtle bugs in your programs. Luckily there are <a href="https://en.wikipedia.org/wiki/Compare-and-swap#ABA_problem">several workarounds</a> for this.</p>

<h4>You can swap anything inside a CAS loop</h4>

<p>The CAS loop is often used to swap pointers, a type supported by the <em>compare-and-swap</em> operation. This is useful when you want to modify a complex collection of data like a class or an array: just create the local copy, modify it as needed and then when ready swap a pointer to the local data with a pointer to the global data. This way global data will point to the memory allocated for the local copy and other threads will see up-to-date information.</p>

<p>This technique allows you to successfully synchronize non-primitive entities, yet is difficult to make it work correctly. What if, after the swap, a reader thread is still reading the old pointer? How to properly delete the previous copy without generating dangerous dangling pointers? Once again engineers have found many solutions such as using a language that supports <a href="https://en.wikipedia.org/wiki/Garbage_collection_(computer_science">garbage collection</a> or esoteric techniques like <a href="https://aturon.github.io/blog/2015/08/27/epoch/">epoch-based memory reclamation</a>, <a href="https://en.wikipedia.org/wiki/Hazard_pointer">hazard pointers</a> or <a href="https://en.wikipedia.org/wiki/Reference_counting">reference counting</a>.</p>

<h3>Lock-freedom vs. wait-freedom</h3>

<p>Every algorithm or data structure based on atomic operations can be clustered into two groups: <strong>lock-free</strong> or <strong>wait-free</strong>. This is an important distinction when you have to evaluate the impact of atomic-based tools on the performance of your program.</p>

<p>Lock-free algorithms allow the remaining threads to continue doing useful work even if one of them is temporarily busy. In other words, at least one thread always makes progress. The CAS loop is a perfect example of lock-free because if a single iteration of the CAS loop fails, it’s usually because some other thread has modified the shared resource successfully. However, a lock-free algorithm might spend an unpredictable amount of time just spinning, especially when there are many threads competing for the same resource: technically speaking, when the <strong>contention</strong> is high. Pushing it to the limits, a lock-free algorithm could be far less efficient with CPU resources than a mutex that puts blocked threads to sleep.</p>

<p>On the other hand in wait-free algorithms, a subset of lock-free ones, any thread can complete its work in a finite number or steps, regardless of the execution speed or the workload level of others. The first snippet in this article based on the <em>fetch-and-add</em> operation is an example of a wait-free algorithm: no loops, no retries, just undisturbed flow. Also, wait-free algorithms are <strong>fault-tolerant</strong>: no thread can be prevented from completing an operation by failures of other processes, or by arbitrary variations in their speed. These properties make wait-free algorithms suitable for complex <a href="https://en.wikipedia.org/wiki/Real-time_computing">real-time systems</a> where the predictable behavior of concurrent code is a must.</p>

<center><p><img src="http://Solarex.github.io/images/lock-free-wait-free.png"></p></center>


<p>Wait-freedom is a highly desired property of concurrent code, yet very difficult to obtain. All in all, whether you are building a blocking, a lock-free or a wait-free algorithm the golden rule is to always benchmark your code and measure the results. Sometimes a good old mutex can outperform fancier synchronization primitives, especially when the concurrent task complexity is high.</p>

<h3>Closing notes</h3>

<p>Atomic operations are a necessary part of lock-free programming, even on single-processor machines. Without atomicity, a thread could be interrupted halfway through the transaction, possibly leading to an inconsistent state. In this article I have just scratched the surface: a new world of problems opens up as soon as you add multicores/multiprocessors to the equation. Topics like <strong>sequential consistency</strong> and <strong>memory barriers</strong> are critical pieces of the puzzle and can&rsquo;t be overlooked if you want to get the best out of your lock-free algorithms. I will cover them all in the next episode.</p>

<h3>reference</h3>

<ul>
<li><a href="https://www.internalpointers.com/post/lock-free-multithreading-atomic-operations">Lock-free multithreading with atomic operations</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Introduction to thread synchronization]]></title>
    <link href="http://Solarex.github.io/blog/2019/08/09/introduction-to-thread-synchronization/"/>
    <updated>2019-08-09T10:15:00+08:00</updated>
    <id>http://Solarex.github.io/blog/2019/08/09/introduction-to-thread-synchronization</id>
    <content type="html"><![CDATA[<p>清空Reeder发现internal/pointers一系列关于多线程的文章，感觉很不错，转载在此。</p>

<p>这是系列第二篇。</p>

<p>A look at one of the most popular ways of concurrency control in a multithreaded application.</p>

<!-- more -->


<p>As emerged from <a href="https://www.internalpointers.com/post/gentle-introduction-multithreading">my previous introduction to multithreading</a>, writing concurrent code can be tricky. Two big problems might emerge: data races, when a writer thread modifies the memory while a reader thread is reading it and race conditions, when two or more threads do their job in an unpredictable order. Luckily for us there are several ways to prevent these errors: in this article I will take a look at the most common one known as <strong>synchronization</strong>.</p>

<h3>What is synchronization</h3>

<p>Synchronization is a bag of tricks that make sure two or more threads behave themselves. More specifically, synchronization will help you to achieve at least two important features in your multithreaded program:</p>

<ul>
<li><strong>atomicity</strong> — if your code contains instructions that operate on data shared across multiple threads, an unregulated concurrent access to that data might trigger a data race. The code segment that contains those instructions is called <strong>critical section</strong>. You want to make sure that critical sections are executed <em>atomically</em>: as defined in the previous episode, an <a href="https://www.internalpointers.com/post/gentle-introduction-multithreading#data-races">atomic operation</a> can&rsquo;t be broken into smaller ones, so that while a thread is executing it no other thread can slip through;</li>
<li><strong>ordering</strong> — sometimes you want two or more threads to perform their job in a predictable order, or put a restriction on how many threads can access a specific resource. Normally you don&rsquo;t have control over these things, which might be the root cause of race conditions. With synchronization you can orchestrate your threads to perform according to a plan.</li>
</ul>


<p>Synchronization is implemented through special objects called <strong>synchronization primitives</strong> provided by the operating system or any programming language that supports threading. You then make use of such synchronization primitives in your code to make sure your threads don&rsquo;t trigger data races, race conditions or both.</p>

<p>Synchronization takes place both in hardware and software, as well as between threads and operating system processes. This article is about synchronization of software threads: the physical counterpart and process synchronization are fascinating topics that will surely get some love in a future post.</p>

<h3>Common synchronization primitives</h3>

<p>The most important synchronization primitives are <strong>mutexes</strong>, <strong>semaphores</strong> and <strong>condition variables</strong>. There are no official definitions for these terms, so different texts and implementations associate slightly different characteristics with each primitive.</p>

<p>Operating systems provide these tools natively. For example Linux and macOS support <strong>POSIX threads</strong>, also known as <strong>pthreads</strong>, a set of functions that allows you to write safe multithreaded applications. Windows has its own synchronization tools in the C Run-Time Libraries (CRT): conceptually similar to POSIX threads functions but with different names.</p>

<p>Unless you are writing very low-level code, you usually want to employ the synchronization primitives shipped with the programming language of your choice. Every programming language that deals with multithreading has its own toolbox of synchronization primitives, along with other functions to fiddle around with threads. For example Java provides the <code>java.util.concurrent</code> package, modern C++ has its own <code>thread</code> library, C# ships the <code>System.Threading</code> namespace and so on. All these functions and objects are based upon the underlying operating system primitives, of course.</p>

<p>There are many other synchronization tools around. In this article I&rsquo;ll stick to the three mentioned above, as they act as a foundation often used to build more complex entities. Let&rsquo;s take a closer look.</p>

<h3>Mutexes</h3>

<p>A <strong>mutex</strong> (<strong>mut</strong>ual <strong>ex</strong>clusion) is a synchronization primitive that puts a restriction around a critical section, in order to prevent data races. A mutex guarantees <em>atomicity</em>, by making sure that only one thread accesses the critical section at a time.</p>

<p>Technically, a mutex is a global object in your application, shared across multiple threads, that provides two functions usually called <code>lock</code> and <code>unlock</code>. A thread that is about to enter the critical section calls <code>lock</code> to lock the mutex; when it&rsquo;s done, that is when the critical section is over, the same thread calls <code>unlock</code> to unlock it. The important feature of a mutex: only the thread that locks the mutex is allowed to unlock it later on.</p>

<p>If another thread jumps in and tries to lock a locked mutex, the operating system puts it to sleep until the first thread has finished its task and has unlocked the mutex. This way only one thread can access the critical section; any other thread is excluded from it and must wait for the unlock. For this reason a mutex is also known as a <strong>locking mechanism</strong>.</p>

<p>You can use a mutex to protect simple actions like a concurrent read and write of a shared variable, as well as bigger and more complex operations that need to be executed by one thread at a time, such as writing to a log file or modifying a database. Anyway, the mutex lock/unlock operations always match the boundaries of the critical section.</p>

<h3>Recursive mutexes</h3>

<p>In any regular mutex implementation, a thread that locks a mutex twice causes an error. A <strong>recursive mutex</strong> allows this, instead: a thread can lock a recursive mutex multiple times without unlocking it first. However no other thread can lock the recursive mutex until all the locks held by the first thread have been released. This synchronization primitive is also known as <strong>reentrant mutex</strong>, where <strong>reentrancy</strong> is the ability to call a function multiple times (i.e. to enter it again) before the previous invocations are over.</p>

<p>Recursive mutexes are difficult to work with and are error-prone. You have to keep track of which thread has locked the mutex how many times and make sure the same thread unlocks it completely. Failing to do so would leave locked mutexes around with nasty consequences. Most of the time a regular mutex is enough.</p>

<h3>Reader/Writer Mutexes</h3>

<p>As we know from the previous episode, multiple threads can concurrently read from a shared resource without harm as long as they don&rsquo;t modify it. So why bother locking a mutex if some of your threads are operating in &ldquo;read-only&rdquo; mode? For example consider a concurrent database that is frequently read by many threads, while another thread seldomly writes updates. You certainly need a mutex to protect the read/write access, but most of the time you would end up locking it just for read operations, preventing other reading threads to do their job.</p>

<p>A <strong>reader/writer mutex</strong> allows <em>concurrent</em> reads from multiple threads and <em>exclusive</em> writes from a single thread to a shared resource. It can be locked in <em>read</em> or <em>write</em> mode. To modify a resource, a thread must first acquire the exclusive write lock. An exclusive write lock is not permitted until all read locks have been released.</p>

<h3>Semaphores</h3>

<p>A <strong>semaphore</strong> is a synchronization primitive used to orchestrate threads: which one starts first, how many threads can access a resource and so on. Like a street semaphore regulates the traffic, a programming semaphore regulates the multithreading flow: for this reason a semaphore is also known as a <strong>signaling mechanism</strong>. It can be seen as an evolution of a mutex, because it guarantees both <em>ordering</em> and <em>atomicity</em>. However in a few paragraphs I will show you why using semaphores for atomicity only is not a great idea.</p>

<p>Technically, a semaphore is a global object in your application, shared across multiple threads, that contains a <em>numeric counter</em> managed by two functions: one that increases the counter, another one that decreases it. Historically called <code>P</code> and <code>V</code>, modern implementations use more friendly names for those functions such as <code>acquire</code> and <code>release</code>.</p>

<p>A semaphore controls the access to a shared resource: the counter determines the maximum number of threads that can simultaneously access it. At the beginning of your program, when the semaphore gets initialized, you choose that number according to your needs. Then, a thread that wants to access a shared resource calls <code>acquire</code>:</p>

<ul>
<li>if the counter is <em>greater than zero</em> the thread can proceed. The counter gets reduced by one right away, then the current thread starts doing its job. When done, it calls <code>release</code> which in turn increases the counter by one.</li>
<li>if the counter is <em>equal to zero</em> the thread cannot proceed: other threads have already filled up the available space. The current thread is put to sleep by the operating system and will wake up when the semaphore counter becomes greater than zero again (that is when any other thread calls <code>release</code> once its job is done).</li>
</ul>


<p>Unlike a mutex, <em>any thread can release a semaphore</em>, not only the one that has acquired it in the first place.</p>

<p>A single semaphore is used to limit the number of threads accessing a shared resource: for example to cap the number of multithreaded database connections, where each thread is triggered by someone connecting to your server.</p>

<p>By combining multiple semaphores together you can solve thread ordering problems: for example the thread that renders a web page in your browser must start after the thread that downloads the HTML files from the Internet. Thread A would notify thread B when it&rsquo;s done, so that B can wake up and proceed with its job: this is also known as the famous <a href="https://en.wikipedia.org/wiki/Producer%E2%80%93consumer_problem">Producer-Consumer problem</a>.</p>

<h3>Binary semaphores</h3>

<p>A semaphore whose counter is restricted to the values 0 and 1 is called <strong>binary semaphore</strong>: only one thread at a time can access the shared resource. Wait: this is basically a mutex protecting a critical section! You can actually replicate the mutex behavior with a binary semaphore. However there are two important points to keep in mind:</p>

<ul>
<li>a mutex can be unlocked only by thread that had locked it first, while a semaphore can be released from any other thread. This could lead to confusion and subtle bugs if what you want is just a locking mechanism;</li>
<li>semaphores are signaling mechanisms that orchestrate threads, while mutexes are locking mechanisms that protects shared resources. You should not use semaphores to protect shared resources, nor mutexes for signaling: your intent will be more clear to you and to anyone who will read your code.</li>
</ul>


<h3>Condition variables</h3>

<p>Condition variables are another synchronization primitive designed for <em>ordering</em>. They are used for sending a wake up signal across different threads. A condition variable always goes hand in hand with a mutex; using it alone doesn&rsquo;t make sense.</p>

<p>Technically, a condition variable is a global object in your application, shared across multiple threads, that provides three functions usually called <code>wait</code>, <code>notify_one</code> and <code>notify_all</code>, plus a mechanism to pass it an existing mutex to work with (the exact way depends on the implementation).</p>

<p>A thread that calls <code>wait</code> on the condition variable is put to sleep by the operating system. Then another thread that wants to wake it up invokes <code>notify_one</code> or <code>notify_all</code>. The difference is that <code>notify_one</code> unfreezes only one thread, while <code>notify_all</code> sends the wake up call to all the threads that are sleeping after the <code>wait</code> call on the condition variable. The mutex is used internally to provide the sleep/wakeup mechanism.</p>

<p>Condition variables are a powerful mechanism to send signals between threads that you couldn&rsquo;t achieve with mutexes alone. For example you can use them to solve the Producer-Consumer problem once again, where thread A emits a signal when it&rsquo;s done so that thread B can start its job.</p>

<h3>Common problems in synchronization</h3>

<p>All the synchronization primitives described in this article have something in common: they put threads to sleep. For this reason they are also called <strong>blocking mechanisms</strong>. A blocking mechanism is a good way to prevent concurrent access to a shared resource if you want to avoid data races or race conditions: a sleeping thread does no harm. But it can trigger unfortunate side effects. Let&rsquo;s take a quick look at them.</p>

<h4>Deadlock</h4>

<p>A <strong>deadlock</strong> occurs when a thread is waiting for a shared variable that another thread holds, and this second thread is waiting for a shared variable that the first thread holds. These things usually happen when working with multiple mutexes: the two threads remain waiting forever in an infinite circular loop: thread A waits for thread B which waits for thread A which waits for thread B which&hellip;</p>

<h4>Starvation</h4>

<p>A thread goes in <strong>starvation</strong> mode when it doesn&rsquo;t get enough love: it remains stuck indefinitely in its sleep state while waiting for access to a shared resource that is continuously given to other threads. For example a poorly designed semaphore-based algorithm might forget to wake up one of the many threads behind the waiting line, by giving precedence only to a subset of them. The starving thread would wait forever without doing any useful work.</p>

<h4>Spurious wake-ups</h4>

<p>This is a subtle problem that comes from how condition variables are actually implemented in some operating systems. In a <strong>spurious wake-up</strong> a thread wakes up even if not signaled through the condition variable. That&rsquo;s why most synchronization primitives also include a way to check if the wakeup signal really comes from the condition variable the thread is waiting on.</p>

<h4>Priority inversion</h4>

<p><strong>Priority inversion</strong> occurs when a thread performing a high-priority task is blocked waiting for a lower-priority thread to release a resource, such as a mutex. For example when the thread that outputs audio to the soundcard (high priority) is blocked by the thread that displays the interface (low priority), resulting in a bad glitch through your speakers.</p>

<h3>What&rsquo;s next</h3>

<p>All these synchronization problems have been studied for years and many solutions, both technical and architectural are available. A careful design and a bit of experience help a lot in prevention. Also, given the <a href="https://www.internalpointers.com/post/gentle-introduction-multithreading#race-conditions">non-deterministic</a>, (i.e. extremely hard) nature of multithreaded applications, people have developed interesting tools to detect errors and potential pitfalls in concurrent code. Projects like <a href="https://github.com/google/sanitizers/wiki/ThreadSanitizerCppManual">Google&rsquo;s TSan</a> or <a href="http://valgrind.org/docs/manual/hg-manual.html">Helgrind</a> are just a few of them.</p>

<p>However, sometimes you want to take a different route and get rid of any blocking mechanism in your multithreaded application. This would mean to enter the <strong>non-blocking</strong> realm: a very low-level territory, where threads are never put to sleep by the operating system and concurrency is regulated through <strong>atomic primitives</strong> and <strong>lock-free data structures</strong>. It&rsquo;s a challenging field, not always necessary, which can boost the speed of your software or wreak havoc on it. But this is a story for the next episode&hellip;</p>

<h3>reference</h3>

<ul>
<li><a href="https://www.internalpointers.com/post/introduction-thread-synchronization">Introduction to thread synchronization</a></li>
</ul>

]]></content>
  </entry>
  
</feed>
